---
layout: post
current: post
cover: assets/built/images/logo-voice.jpg
navigation: True
title: NAVER AI NOW
date: 2021-05-25 13:40:00
tags: [NLP]
class: post-template
subclass: "post tag-NLP"
author: egeg1212
---

## NAVER AI NOW 2021.05.25

<br><br>

|             **온라인 컨퍼런스**              | [NAVER TV_NAVER AI NOW]](https://tv.naver.com/v/20349478)       |
| :------------------------------------------: | :-------------------------------------------------------------- |
|              **모두를 위한 AI**              | <https://naver-ai-now.kr/>                                      |
|                                              |                                                                 |
|                  **PART1.**                  | **HyperCLOVA 커다란 카능성을 열다.**                            |
|    성낙호<br>NAVER CLOVA Biz AI 책임리더     | 새로운 AI의 시작, HyperCLOVA                                    |
|          황인용<br>NAVER Cloud 리더          | HyperCLOVA를 위한 슈퍼 컴퓨팅 인프라                            |
|     강인호<br>NAVER Search CIC 책임리더      | HyperCLOVA를 위한 Big Data                                      |
|       하정우<br>NAVER AI LAB 책임리더        | 새로운 글로벌 AI R&D 리더십                                     |
|           송대섭<br>NAVER 책임리더           | AI, 사람을 위한 일상의 도구                                     |
|                                              |                                                                 |
|                  **PART2.**                  | **HyperCLOVA테크놀로지**                                        |
| 박우명, 김보섭, 김형석<br>CLOVA Conversation | HyperCLOVA의 한국어 모델                                        |
|            장민석<br>NAVER AI LAB            | HyperCLOVA Studio 나에게 필요한 인공지능, 내 손으로 쉽게 만들기 |

### 새로운 AI의 시작, HyperCLOVA

GTP-3한국어 데이터 대비 6,500배
기존NAVER언어모델 데이터 대비 3,000배
뉴스 50년치에 해당하는 크기
네이버블로그 9년치에 해당하는 크기
=**한국어 데이터 5,600억 토큰**

**비지도학습**

**인공지능의 성능**은
학습에 사용되는 데이터의 양,
연산의 규모,
모델의 파라미터 수.
3가지요소 모두 서로에게 병목이 되지 않는다는 전제 하에서
그 성능이 무한히 향상될 수 있음. (참조: Scaling Laws)
많은양의 데이터를 효과적으로 사용하기 위해

1. 맥락을 이해하는 자연스러운 대화

   - 이전 질문/응답 내용 이해하여 다음 대답
   - 사용자의 만족도 인지, 호응, 감정표시
   - 20회이상 주고 받는 대화

2. 창작을 도와주는 글쓰기
3. 정보요약

   - 주제에 대한 여러 의견 요약하여 빠르게 이해하도록 도와줌

4. 데이터 생성
   - 전화를 대신 받아주는 CLOVA AiCall을 만들기 위해 HyperCLOVA가 쓰임.

대화 시나리오 구축 생산성 x10배

의도 기반 검색하기/영화제목 맞추기/시,소설 쓰기/ 번역하기/캐릭터와 대화하기/앱디자인하기/법률문서간소화/엔티티인식하기/셀럽말투따라하기/웹페이지만들기/콘텐츠분류하기/마케팅문구생성하기/가사쓰기/의료증상구분하기/토론질문생성하기/끝말잇기게임하기/자동으로 메일으씩/레시피만들기/비속어필터하기/백과사전묻고답하기.문법교정하기/사투리변환하기/역사적인물과대화하기

확장예정
글 음악 그림 음성 비디오 코드

### HyperCLOVA를 위한 슈퍼 컴퓨팅 인프라

Big Model을 위해 구축된 슈퍼 컴퓨팅 인프라의 소개와 향후계획
(현.날씨예보, 과학시뮬레이션)
CLOVA Chatbot
CLOVA OCR
CLOVA AiCall
슈퍼컴퓨터 병렬 학습을 통한 문제해결
BIG AI를 위한 핵심 컴퓨팅 자원인 슈퍼컴퓨터

인프라규모 :
Computing node140개, GPU 1120개, 케이블3800개

기술요소 :

1. **고성능의 병렬 GPU클러스터**와 초저지연 인피니 밴드 네트워크 & 스토리지 기술로 Big Model의 복잡하고 긴 학습 시간을 단축시키고, 빠르게 모델을 구축하도록 도와줌.
2. **초저지연 고대역폭 네트워크**
   인피니티밴드 기술(많은 슈퍼컴퓨터들이 사용하는 네트워크기술. 분산컴퓨팅 환경에서 OS를 통하지 않고 네트워크를 통해 노드간 메모리를 직접 읽고 쓸 수 있게 한(RDMA)오버헤드 없이 초저지역 고대역폭을 극대화.)
3. **고성능 병렬 아키텍처 기반의 스토리지**
   고대역폭의 대규모 서버에서 동시 데이터 엑세스가 가능함으로써 대규모 워크 로드 처리가 가능.
   데이터를 GPU 메모리로 직접 전송(GDS)하여 성능을 더욱 높일 수 있다.
   일반 네트워크 스토리지보다 2배 이상의 성능을 갖고 있으며, 확장에 따라 성능을 더울 높일 수 있다.

**인프라 운영 노하우**

1. 클라우드 인프라 운영 역랑 :
   슈퍼컴퓨팅 클러스터가 내부 인프라 운영 표준 환경과 연계될 수 있도록 빠르게 최적화.
2. 자체 데이터센터 구축 노하우 :
   랙 설비, 네트워크 구성, 관리 시스템 연동 등 전체적인 인프라 구축 일정을 단축
3. 모니터링 플랫폼과 운영자동화 :
   최소한의 서비스중단, 연속성 보장을 위해, 모니터링을 비롯한 자체 관리 플랫폼과 운영 자동화 솔루션

### HyperCLOVA를 위한 Big Data

대용량 데이터 준비시 고민해본 부분 4가지.

1. 다양한 내용(일상생활에서 접할 수 있는 뉴스,카페,블로그,지식인,웹문서, 오픈된 리소스, 전문지식)
   문서 내용이 유사한 경우 중복 제거(치우치지 않기위해)
   개인정보(전번,메일)는 제거하거나 비식별화 처리
2. 범용의 구성(검색, 대화, Q&A, 요약등 여러 생성 작업 포함)
   메타정보 추가(대화문에서의 화자ID)(문서의 카페명, 블로그명 같은 출처정보, 카테고리정보 추가)
3. 양질의 정보
   영역선별-정보성이 있으면서 신뢰도 있고, 인기 있는 공식 사이트 출처가 상위 품질에 포함되도록 함.
   상위 품질의 문서에서도 정보의 가치나 유용성에 따라 선별작업 진행(웹 페이지 내 핵심 영역만 사용 - **행심 영역을 판정하는 기계학습 모델을 만듦**)
   저품질 문서 필터링 : 의미없는 단어의 나열, 비속어나 유해 정보 제거, 서비스별 홍보 또는 스팸판별 결과 활용
4. 충분한 크기
   1.96TB 데이터셋
   한국어데이터 5,600억 토큰 = 뉴스 50년치에 해당하는 크기 = 네이버블로그 9년치에 해당하는 크기 = 한국어 위키피디어 2,900배

한국의 지식과 한국어의 특성을 잘 반영한 좋은 구성이다.
텍스트 위주의 데이터 구축에 이어,
앞으로 문서와 관련된 그림, 음성, 비디오 등 멀티모달리티 측면에서도 데이터 구축을 준비하고 있다.

### 새로운 글로벌 AI R&D 리더십

구글, 페이스북, OpenAI등과 같은 글로벌 기업들은
지난 수년간 의미있는 기술들을 공개 해왔고
많은 기업들이 이런 기술들을 잘 활용하고 있다.

그러나 글로벌 빅테크 기업들이 공개한 기술들을 그냥 적용하는 것 중심으로 하는 전략도 효율성이 있겠지만,
기술경쟁력 관점에서의 한계도 존재.

서울대 X NAVER SNU
KAIST X NAVER
그 외

### AI, 사람을 위한 일상의 도구

> 사용자의 일상을 편리하게 만들어 줄 수 있는 도구

NAVER AI 윤리 준칙 5개 조항 1. 사람을 위한 AI 개발 Developing Human-centered AI 2. 다양성의 존중 Respecting Diversity 3. 합리적인 설명과 편리성의 조화 Balancing Reasonable Explainability with Convenience 4. 안전을 고려한 서비스 설계 Accounting for Safety in Service Design 5. 프라이버시 보호와 정보 보안 Protecting Privacy and Data Security

ex1)
코로나일상에서 국민의 건강과 안전을 케어하는 도구
(**클로바 케어콜**. 성남시10만건2020.03-2021.02)
(그 외 10개 지방자치단체 도입. 서울, 고양, 의정부, 춘천, 인천, 성남, 화성, 수원, 전주 부산)
SME의 사업을 돕는 CLOVA AiCall 전화예약
AI기술의 집약체 **CLOVA AiCall** - 자연어처리 - 음성인식 - 음성합성 - 텍스트 분석

ex2)
즐거운 독서 경험을 제공하는 도구 **클로바램프** - 문자인식 - 이미지인식 - 음성합성 - 음성인식 - 자연어 처리

### HyperCLOVA의 한국어 모델

#### [Language Model] 네이버에서 한국어 모델을 만든 이유 - 박우명Conversation팀

현 GPT-3는 학습 데이터 구성상 한국어 성능이 제한적이다.
인터넷언어 분포 :
**영어60.3%** > 기타,독일어,프랑스어,중국어,일본어,스페인어,러시아어 > **한국어 0.6%**

GPT-3 언어 분포
**영어92.7%** > 기타,독일어,프랑스어,중국어,일본어,스페인어,러시아어 > **한국어0.1%**

비중은 작아도 과거 하지 못했던 일부 난이도 높은 태스크가 동작하기도 하는 등 뛰어난 성능을 보이고 있다.
이러한 상황에서
**만약 한국어에 맞는 Big Model을 확보하지 못한다면,
글로벌 기업들이 강력한 영어 모델을 기반으로 다양한 서비스를 만들어 낼 때
우리는 그들이 제공하는 제한적인 한국어 성능의 모델을 사용할 수밖에 없게 되고
이는 기술이 종속되는 현상뿐 아니라 한국어 기반 서비스의 성장에 명확한 한계로 작용하게 될 것이다..ㅠㅠㅠ**

GPT-3와 비슷한 규모로 데이터를 학습함.
**데이터 처리 파이프라인**
데이터 전체560B토큰 중 300B 토큰학습함.
어휘 집합 학습: 한국어에 최적인 토크나이징Tokenizing방법 학습.

**전처리를 효율적으로**
코퍼스믹서Corpus Mixer : 전처리 시 데이터 종류별 비율 자동 조절
(뉴스, 구어체 등등 )
시리얼라이저Sericalizer : 하둡 스트리밍 적용/ 처리속도 약 170배 개선(7일이 1시간으로 줄어듦)

**모델의 크기를 키우려면**
생성 모델로 다양한 태스크에 활용 가능성
모델 크기 증가에 따라 GPU 1장으로 학습 불가
3중 병렬화 적용 : 데이터, 모델, 파이프라인

성능테스트 :다운스트림 태스크
NSMC
KorQuAD1.0 한국어 Machine Reading Comprehension데이터셋

#### [Tokenization] HyperCLOVA가 한국어를 읽는 방법:토큰화(Tokenization) - 김보섭Conversation팀 10:30

1. 기계가 글을 이해하려면?
   기계가 문장을 이해하기 위해 단위Token으로 문장을 끊어 읽는 능력이 필요함.
   방법1- 어절단위의 경우: 문장을 적은 토큰 개수로 처리할 수 있지만 어휘집합(Vocabulary)을 크기를 만들지 못해서 토큰이 다양하지 못해 처리할 수 없는 문장이 발생하기도 함.
   방법2- 문자단위의 경우: 문장을 처리할 때, 어절 단위에 비해서 더 많은 토큰 개수를 요구하지만 처리할수없는 문장이 덜 발생함 ㅋ - 방법 1,2의 장단점을 보완할 수 있는 방법은?
   **서브워드Subword(어떤단어의부분)로 끊어 읽으면** 규칙 기반의 장점들을 취하면서 단점을 줄일 수 있으며, 데이터를 기반한 알고리즘**BPE**을 통해 학습가능함.
   **BPE(Byte-pair encoding) 정보 압축 알고리즘.**메모리를 많이 필요로 하는 작업;
   자주 등장하는 문자열을 하나의 문자열로 병합해 가면서 어휘 집합을 구성하는 방법. - 말뭉치로부터 병합의 대상이 되는 문자열에 적절한 전처리를 수행함으로써 서브워드를 학습하는 방식에 변화를 줄 수 있음
   character->Byte->Morpheme-Aware Byte
2. 대용량 말뭉치로 서브워드 토크나이저 학습하기
   2TB말뭉치를 모두 사용하는건 현실적으로 불가능하다.
   **전체 말뭉치를 대표할 수 있도록 샘플링Sampling해서 서브워드 토크나이저학습에 사용하는 것이 중요함** - 적절한 양의 말뭉치를 결정하기 위해 2가지 가설을 토대로 말뭉치선정함. 1. 일반적으로 자연어 말뭉치에 사용된 단어를 빈도순으로 나열했을 경우 지프의 법칙(Zipf's Law)을 따름(and, the, I, to, a, of, my, is, that) 2. 1에 따라 학습에 사용한 말뭉치 양에 차이가 있어도 서브워드 토크나이저(Subword Tokenizer)의 어휘집합(Vocabulary)에는 중복되는 토큰(Token)이 많을 것이다.
   **_말뭉치의 양을 줄여가면서 서브워드 토크나이저를 학습하고,
   중복되는 토큰의 비율을 검사하여 최종적으로 사용할 말뭉치의 양을 결정함._**
   최종적으로 전체 말뭉치의 1%(20G)를 사용하여 서브워드 토크나이저를 학습함.
   그래도!!!!!!! 메모리부족(OUT of Memory)과 같은 이슈들이 발생.....전처리를 통해 해결!!
3. 언어모델을 위한 서브워드 토크나이저
   - 3가지 BPE를 테스트한 결과
     미등록 단어 문제가 발생하지 않으며, 고빈도 형태소를 기반으로 학습하는 Morpheme-Aware Byte-Lavel BPE가 적합할 것이라고 판단함.
   - 좋은 서브워드 토크나이저로 만든 언어모델이 생성한 문장은 사람의 문장과 비슷하다고 가정 -> 지표화를 통해 **판별 모델**을 도입.(토큰화의 방식이 달라지면, 언어모델을 비교할때 많이 사용하는 퍼플렉시티Perplexity지표를 사용할 수없다.)
   - '더 좋은 서브워드 토크나이저로 만든 언어 모델은 더 사람이 쓴 것 같은 문장을 생성한다'고 가정
4. HyperCLOVA가 글을 읽는 방법(한 문장으로 정리)
   언어모델은 학습용 말뭉치의 1%로부터 학습된 Morpheme-Aware Byte-Level BPE Tokenizer로 문장을 처리함

#### [Metrics] HyperCLOVA 한국어 능력 평가 - 김형석Conversation팀17:39

1. 설계목적
   "모델이 생성한 문장은 얼마나 유창한가Fluency"
   이를 측정하기 위해, 정량화 수치화
2. 문제점들
   - 생성 문장과 레퍼런스 문장 간의 유사성이 문장 품질을 보장하지 않음
   - 서로 다른 설정(특히 어휘 집합)에서 학습한 모델들을 Preplexity(PPL)로 비교하는것이 부적절함
3. 개선 아이디어
   - 래퍼런스 없는 평가지표
     레퍼런스 의존성이 없으면서 loss가 아닌 생성텍스트에 기반한 품질 평가 지표여야 함.
   - 생성 문장 기반 평가지표
4. 평가결과
