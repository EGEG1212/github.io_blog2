{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting theano\n",
      "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.5.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.15.0)\n",
      "Building wheels for collected packages: theano\n",
      "  Building wheel for theano (setup.py): started\n",
      "  Building wheel for theano (setup.py): finished with status 'done'\n",
      "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668111 sha256=f6c4e4bb7846dec1655120669243c4f51b659b275e14cce4efd4a0f9f00de242\n",
      "  Stored in directory: c:\\users\\pc-go\\appdata\\local\\pip\\cache\\wheels\\84\\cb\\19\\235b5b10d89b4621f685112f8762681570a9fa14dc1ce904d9\n",
      "Successfully built theano\n",
      "Installing collected packages: theano\n",
      "Successfully installed theano-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = numpy.random\n",
    "\n",
    "N = 400\n",
    "feats = 784 #차원\n",
    "D = (rng.randn(N, feats).astype(numpy.float32), rng.randint(size = N, low=0, high=2).astype(numpy.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-0.03968759,  0.32623804, -0.2864035 , ...,  1.0145822 ,\n",
       "         -0.98714745,  0.68108976],\n",
       "        [ 0.70494425,  1.1646211 ,  1.0447122 , ...,  0.11065981,\n",
       "         -0.92154187,  0.7028902 ],\n",
       "        [ 1.7221528 ,  0.23708415,  1.9193072 , ...,  2.097736  ,\n",
       "         -1.3243518 ,  0.21220408],\n",
       "        ...,\n",
       "        [ 0.5444983 ,  0.7940225 ,  0.04632351, ..., -0.8444883 ,\n",
       "         -1.1282709 ,  2.1386142 ],\n",
       "        [-0.15034786, -0.29176366,  0.18123117, ..., -0.0937378 ,\n",
       "         -0.11165322,  0.17779373],\n",
       "        [-0.8927488 , -0.92266774,  0.7283412 , ...,  0.8297746 ,\n",
       "         -0.3400725 ,  0.63995266]], dtype=float32),\n",
       " array([1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 0., 0.], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "D\n",
    "#가우시간분포에 따른. 400개의 레이블이 달려있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.91193345  0.63892273 -0.71941473 ... -1.33527277  0.73837072\n   1.38924837]\n [ 0.5581504  -0.30152718  0.32769058 ...  1.17244099 -0.81116815\n   1.04825718]\n [ 1.05933354  2.05137907 -1.96551677 ... -0.74216618 -2.03028449\n   0.55809047]\n ...\n [ 0.53049583  0.02157492 -1.02954589 ...  0.774203    0.89361168\n   1.32411742]\n [ 0.53819113 -0.25902789  0.50222782 ... -1.29481319 -0.42896654\n  -0.3317319 ]\n [-0.80546122  1.31027997 -0.14003127 ... -0.64395619 -1.70540192\n  -0.90070639]] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[-0.18668918 -0.84287693 -0.62107179 -0.38955162  0.55726845  0.38801171\n -0.41565086  0.31710532 -0.13416397 -0.65241825 -0.82463423 -0.23421881\n  0.72658131  1.3073682   2.64901381  1.15010926 -1.01638705 -0.22045888\n  0.61185981  1.11853618 -1.33623058  0.74660065 -0.39294644 -0.78453651\n -0.39910608  0.06191575 -1.46316714 -0.08447413 -0.08251807 -0.86530487\n -0.92312007 -0.55360073  0.40336459  1.41016883  0.77803133 -0.2330329\n -0.20553976 -0.66806649 -1.24960761 -1.01886316  2.34786331 -0.66627363\n -1.51388646  0.89149262  1.80332222  1.29113601 -1.65079471 -0.13486959\n -1.02463002 -0.47626569 -0.19650702 -0.10514686 -1.01205288  1.17046906\n  0.18645894 -0.01857437  2.19904279 -0.284601   -0.01885763 -0.13486807\n  1.27402254 -0.10534138 -0.72832046 -1.25841163 -0.61727362  0.55019468\n -0.35376909 -0.17884404 -0.78279627 -0.75773709  0.38234021  0.22101313\n -0.80383725  1.32707493  0.31919838 -0.91754569 -0.34284874  0.2826746\n -0.41770645  0.47674479  0.21426539  0.72135939  2.41932669 -0.23203883\n  0.32549192 -0.99858767 -1.00270865  0.01281052  0.50507095  0.4107643\n  0.64248066 -0.3260234   0.87652222 -1.07318845 -0.0955651  -0.1059995\n  0.43294392 -2.75273846  0.08381457 -0.20769605  0.05330276  0.34260298\n  0.75050665  0.01217291 -0.88170625 -0.34603999 -0.76647721 -1.47359974\n -0.08534046 -0.50192779  1.40289497 -2.37618241 -1.40670998  0.7062895\n  0.36752632  1.9913155   0.4278721   0.57014159 -0.98087354 -1.53564191\n -0.83472914 -0.25432482  1.93663723  0.33851495 -0.17883038 -0.88782433\n -1.26227395 -0.39090793 -0.83597432  2.43509566 -1.28385385  0.67188589\n -0.30382091 -0.21858016 -0.44038565  1.96535295  0.17666942  1.57673968\n  0.93496532 -0.30029693  1.47397832  1.43872158 -0.43640247  0.5187453\n  0.86266746  0.80947124  0.25923412  0.0189863  -0.46212837 -1.33234516\n -1.68033912 -1.2579394   0.564451   -0.87452256 -0.67885468  0.02359229\n  1.77610082  1.01878578  0.05451306  0.05194913 -0.28976428  0.45124914\n  1.86150476 -0.11718844  0.78467436  0.68519158 -0.66257112  0.44038051\n -0.2678314  -0.2636233  -1.40891744 -0.37592289  0.52444636 -2.68875469\n  0.28257717  0.91719716  0.11611303  1.6835271  -1.03101234  1.18691191\n  1.27362555  0.95989182 -0.59217916 -0.1049146   1.04830654 -0.1967342\n  0.94057648 -1.29558974  0.43529339  1.51777237  0.07817532 -0.37100327\n  2.85885546  0.62178436 -0.19299101 -0.02464568 -0.26478711 -0.53034814\n  1.15452862  0.03023015 -0.18166217  0.42803472 -0.55355049 -1.06412876\n  0.76501006  0.44134338 -0.19750494  1.54293112 -0.1911823  -2.4582198\n  0.26592896 -0.82499026  1.51250465 -0.1935144  -0.73414461  0.48935829\n -1.43289387 -0.77055129 -1.39309467 -0.69066213 -0.74870051 -1.19690244\n -1.59806868  0.66631091  1.41062019  1.11965239 -0.34778778 -0.12351565\n  0.26707622 -0.08167269  0.25625628  0.10365456 -0.16546265 -1.0458382\n  0.23507325  0.0517782  -0.16110192 -0.87503303  0.37318971 -0.22448335\n -0.5880142   1.17517055  0.07055411  0.72727295  1.39790342  0.79575813\n -0.67769541 -1.00990614  0.06877186  0.41363478 -1.25689165 -0.45313471\n  0.14645274 -1.64158407  1.17344799  1.00870674  0.83086912 -1.20219302\n  1.27729154 -0.36795035 -0.37574463 -0.15076182  0.95824902  1.1403624\n  0.46554717  1.73263265  0.19278916  0.42909698 -0.37054964  1.63584331\n  0.23037314 -0.27905685 -0.56433865 -0.38486382  0.49369047 -1.39960878\n  0.14618188  0.29313808 -0.93955091 -0.06561327 -0.09969509 -0.29628194\n  0.14064006 -0.01432175 -0.43597951 -0.52217526  0.56937455 -1.1013863\n -1.31498826 -0.01736189 -1.73708197 -0.80822594  0.22389792 -0.44893354\n -0.02526708 -0.01471571 -1.21739481  0.20709959 -0.06014108  1.64327828] 0.0\n"
     ]
    }
   ],
   "source": [
    "#3층으로 이루어진 다층퍼셉트론의 파라미터 지금은 GPU에 있다\n",
    "training_steps = 10000\n",
    "x = T.matrix('x')\n",
    "y = T.vector('y')\n",
    "w_1 = theano.shared(rng.randn(784, 300), name = 'w1')\n",
    "b_1 = theano.shared(numpy.zeros(300,), name = 'b1') # 입력층784-은닉층300 사이 가중치\n",
    "w_2 = theano.shared(rng.randn(300,), name = 'w2')\n",
    "b_2 = theano.shared(0., name = 'b2') # 은닉층-출력층 사이 가중치\n",
    "\n",
    "#get_value()란, GPU VRAM에 있는 데이터를 CPU RAM으로 가져와서 비주얼해줌\n",
    "print (w_1.get_value(), b_1.get_value())\n",
    "print (w_2.get_value(), b_2.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 출력값을 출력\n",
    "p_1 = T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(x, w_1)+b_1), w_2)+b_2)\n",
    "prediction = p_1 > 0.5 # 0또는 1일텐데, 0.5보다크면 1로 분류하라\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) #크로스엔트로피 정의\n",
    "cost = xent.mean() + 0.01 * ((w_1**2).sum()+ (w_2**2).sum() + (b_1**2).sum() + (b_2**2).sum()) #에러값정의 모델의 파라미터값이 커지지 못하도록 미분\n",
    "gw_1, gb_1, gw_2, gb_2 = T.grad(cost, [w_1, b_1, w_2, b_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-23-6055d0711dc2>:1: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n  train = theano.function(inputs=[x,y],\n"
     ]
    }
   ],
   "source": [
    "train = theano.function(inputs=[x,y],\r\n",
    "                outputs= [prediction, xent],\r\n",
    "                updates= {w_1 : w_1-0.1*gw_1, b_1 : b_1-0.1*gb_1,\r\n",
    "                            w_2: w_2-0.1*gw_2, b_2 : b_2-0.1*gb_2}) # GPU에서 CPU로 가져오기 update / running rate 0.1 학습률\r\n",
    "predict = theano.function(inputs=[x], outputs=prediction) # 예측함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final model:\n[[-0.00250912 -0.00446023  0.00137025 ... -0.00192448  0.00369112\n   0.00242716]\n [-0.00353448 -0.00635199  0.00192308 ... -0.00270506  0.00523063\n   0.00341786]\n [ 0.00621797  0.01109977 -0.00339112 ...  0.00476536 -0.0091678\n  -0.00601409]\n ...\n [ 0.00530576  0.00952523 -0.00288791 ...  0.00406158 -0.00784736\n  -0.00513087]\n [ 0.00055254  0.00090462 -0.00031055 ...  0.00043094 -0.0007767\n  -0.00053588]\n [ 0.00455305  0.00811802 -0.00248432 ...  0.00349036 -0.00670839\n  -0.00440396]] [ 1.80210137e-04  9.82381217e-04 -2.85690117e-05  9.33848281e-04\n -1.21721973e-06 -2.52283786e-05  1.00483858e-03 -6.76835053e-04\n  1.14928243e-03  2.69155924e-04  2.99313868e-04  1.36353004e-03\n -3.72283151e-04 -8.62078821e-04 -1.63798854e-03 -1.38307546e-03\n  9.50266902e-04 -2.91169525e-07 -5.00489039e-05 -6.53918521e-04\n  8.01814238e-04 -4.52912678e-06  1.71377561e-03  1.07201084e-06\n  8.41103211e-05 -1.99142946e-04  2.10469734e-04 -7.71616702e-06\n  2.11799075e-07  1.55472370e-04  8.76165559e-04  1.82409072e-04\n  5.97624263e-07 -3.51530056e-05 -1.12641638e-03  2.89698106e-09\n  2.86257336e-05  1.36912067e-03  3.94124833e-04  2.03962877e-04\n -1.92956944e-03  3.56670151e-05  1.35528283e-03 -9.36589580e-04\n -7.37802423e-04 -8.34978625e-05  1.94327191e-04 -3.10773785e-05\n  1.72720180e-03  2.17846076e-05 -8.02093930e-07  7.84529869e-04\n  9.15062686e-04 -1.01160191e-03 -2.72917069e-04  1.57518570e-04\n -5.66054379e-04 -1.47934806e-07  3.06328719e-07 -3.10769675e-08\n -1.27405065e-03 -6.59201049e-04  5.05067695e-04  2.36801923e-04\n  1.14799714e-05 -5.14885841e-07 -5.05005342e-06  3.54324406e-04\n  5.38750796e-05  4.38819923e-06 -1.95690785e-04  1.64088395e-04\n  6.21208235e-04 -1.20844032e-04  1.01348351e-03  1.32503810e-04\n  4.40501171e-08 -1.09750797e-03 -6.56457749e-05  2.11119950e-08\n  2.95631385e-04 -1.95230313e-05 -4.02609987e-04 -1.09042039e-04\n  2.47370744e-05  2.79844325e-04  3.98237975e-05  1.08865431e-04\n -9.07488957e-04 -2.49908051e-05  1.48145714e-04 -9.02600396e-05\n -1.87204897e-03  4.77769829e-06  8.46982792e-06  2.41766033e-04\n -6.28493828e-05  1.89181080e-03 -1.66088919e-05  5.49870039e-05\n -4.68992389e-04 -1.52800918e-06 -5.36796608e-05  6.65867298e-04\n  1.48077034e-03  1.74396094e-05  1.10583937e-03  1.50234348e-05\n  3.27657053e-06 -1.05521715e-04 -1.99920906e-03  1.06834282e-03\n  1.17120475e-03  1.47488576e-07 -5.37265768e-05 -8.00931320e-04\n -1.33267796e-07 -2.79979139e-05  1.08642807e-06  1.31858895e-03\n -1.35554500e-07 -1.68771464e-06 -1.18771631e-03 -4.33068577e-05\n  2.10104096e-07  5.47466014e-04  1.16299684e-04 -2.55085821e-08\n  6.02385164e-04 -1.74430578e-03  2.03880940e-04 -5.17474858e-05\n  7.85228831e-04 -4.83644126e-07  1.75705837e-06 -9.77557931e-04\n -2.80569155e-05 -3.87219623e-04  1.17611231e-06  3.44215253e-04\n -5.55072702e-04 -2.16480263e-05  6.27786588e-04 -5.49738262e-04\n -6.90947400e-04 -1.94703642e-04  7.04456030e-06 -1.21107690e-05\n  1.36727862e-04  2.36736995e-04  3.58242288e-05  1.18202231e-07\n -4.55464243e-04  6.02961878e-05 -1.21296699e-05 -3.73175145e-04\n -1.69673372e-04 -1.42159542e-03  9.27543116e-06 -6.58427506e-05\n -1.17406536e-05  5.07911695e-04 -1.33036436e-03  2.04375254e-05\n -2.99724610e-04 -7.41522608e-07 -3.16017042e-06 -3.07527800e-04\n  5.92644725e-04  9.04179692e-04  1.88846577e-03  4.77726026e-05\n -4.78034484e-04  1.90499775e-03 -2.81480861e-04 -5.38375821e-06\n -3.61450645e-08 -7.79055024e-04 -1.45798936e-07 -3.52234823e-04\n -6.64323804e-04 -1.67700492e-03  2.16401959e-05  2.35490476e-04\n -1.13317204e-04  1.45058173e-07 -7.68512168e-04  9.03677038e-04\n -1.40931972e-03 -1.33582845e-04  2.17667685e-05  8.17414317e-04\n -4.16468186e-04 -5.99800242e-04  2.61025405e-04 -1.41835720e-06\n  3.13918461e-04 -8.96468662e-05 -6.83145685e-04 -3.39331705e-04\n  5.25999877e-05 -2.05424606e-05  1.10635376e-04  8.43792586e-04\n -2.56279658e-05 -1.02278316e-03 -8.07593765e-04 -7.13394505e-06\n  1.43690208e-04  1.39782814e-04 -8.48845920e-04  1.86111051e-04\n -1.31140563e-03 -1.44294661e-05  5.22363843e-04 -2.01075055e-07\n  1.41263084e-03 -2.32532840e-05  1.10217676e-04  5.62854159e-04\n -5.30374857e-08  1.07300472e-03  2.42708790e-06 -1.27630597e-04\n -1.19216404e-03 -3.69731556e-04  5.64997977e-05  3.57085796e-05\n  3.80153667e-04  9.35469675e-05 -6.61463611e-04 -4.32730480e-04\n -3.87306767e-04  3.65434302e-04 -1.35149477e-04  1.20427004e-04\n  5.16197468e-05  1.60721282e-04 -1.65297607e-04  3.81030726e-05\n  1.11880413e-03 -8.92423556e-05 -5.45442150e-08 -2.75407364e-05\n -1.34348307e-06  5.75088422e-06  4.06601874e-06  3.66103440e-06\n -6.10186399e-08 -1.25229066e-05  1.29949856e-04  2.74221201e-06\n -2.55933407e-06  1.37882551e-03 -1.03714385e-03 -6.38852862e-04\n -1.17832682e-03  7.67590476e-04 -1.04790422e-03  4.25836009e-04\n  8.67545109e-04  4.14872433e-05  1.48795007e-07 -2.75076873e-05\n -6.00657873e-04 -5.15878478e-04 -1.78826035e-04 -8.94073133e-04\n  5.81810395e-08 -1.04094114e-03  1.96013374e-04  5.01848394e-05\n  7.62899208e-04 -8.19656090e-05 -8.73451911e-05  6.84064819e-04\n -5.52927778e-04 -5.41307475e-05  9.71008392e-09 -6.95614142e-04\n  5.05844539e-05  8.16179916e-05 -2.13928902e-04 -1.07902931e-04\n -2.15391320e-06  5.94781058e-04  5.29085791e-06  3.72115643e-04\n  1.24476736e-03  2.63867038e-04  1.93223577e-03  5.88770003e-05\n  2.51231824e-05  9.48821914e-04 -1.53902225e-03 -7.09096134e-06\n  5.22262696e-05  8.10187346e-05 -5.65805693e-04 -1.63036068e-04]\n[-0.13881441 -0.24851605  0.07566882 -0.24411576  0.02988222  0.07269798\n -0.2505078   0.21817896 -0.26272175 -0.15894757 -0.1647828  -0.27925825\n  0.17757301  0.23741808  0.29841796  0.28075859 -0.24561947  0.02232241\n  0.09079839  0.21558276 -0.2313931   0.04271705 -0.30328681 -0.02889975\n -0.10764786  0.14363306 -0.14626281  0.0501069  -0.02122587 -0.13210151\n -0.23870171 -0.13938183 -0.02536066  0.08092137  0.26092295 -0.0176506\n -0.07564388 -0.27966844 -0.18099571 -0.14472231  0.31677101 -0.08123056\n -0.27865129  0.24443899  0.2248327   0.10746046 -0.14238211  0.0777543\n -0.30414859 -0.06928222  0.02709656 -0.22963525 -0.2423752   0.2511734\n  0.15976966 -0.13268207  0.20506889 -0.01050307 -0.02242134 -0.00140961\n  0.27262405  0.21618607 -0.19708686 -0.15220272 -0.05656793  0.02468901\n  0.04410983 -0.17452919 -0.09294368 -0.0422485   0.14278999 -0.13451296\n -0.21170575  0.12150005 -0.25126732 -0.12521977  0.00205991  0.25852636\n  0.09925685  0.00099001 -0.16409099  0.06696972  0.18239228  0.11741205\n -0.07216692 -0.16106118 -0.08419736 -0.11727611  0.24173869  0.07247683\n -0.12998223  0.11026813  0.31328263 -0.04331841 -0.05148211 -0.1532742\n  0.09784488 -0.31441786  0.06361584 -0.09356919  0.19219439  0.03163527\n  0.09290595 -0.21687185 -0.28766348 -0.06453471 -0.25915    -0.06155623\n -0.03882682  0.11613758  0.32091674 -0.25599906 -0.26449337  0.00940766\n  0.09293238  0.23137514 -0.01266732  0.07517744 -0.02899344 -0.27592425\n -0.01246968  0.03245753  0.26588578  0.0866049  -0.02120304 -0.20264496\n -0.1198854  -0.01697154 -0.20945945  0.30531119 -0.14470284  0.09179621\n -0.22970686  0.02439101 -0.03272524  0.24815574  0.07522864  0.17997644\n -0.02955507 -0.17281416  0.20368509  0.06921611 -0.21248088  0.20300689\n  0.21975095  0.14254731 -0.04866402  0.05759529 -0.12654022 -0.15218847\n -0.08134691 -0.01984071  0.1902714  -0.0964467   0.05762385  0.17771841\n  0.13610525  0.28354231 -0.05294341  0.09935485  0.05703994 -0.19746854\n  0.27687485 -0.0678814   0.16493181  0.02663069  0.03850465  0.16637876\n -0.20827995 -0.24135718 -0.31421433 -0.08935301  0.1934605  -0.31521816\n  0.16145241  0.04495511 -0.00164568  0.22914499 -0.00906674  0.17424944\n  0.21676837  0.30097677 -0.06913468 -0.15191736  0.11892461  0.01150788\n  0.22805635 -0.2413101   0.28266005  0.125632   -0.06926408 -0.23295965\n  0.18451835  0.20921907 -0.1573036   0.03104212 -0.1674744   0.11001933\n  0.21888444  0.17204617 -0.09221618  0.06806603 -0.11790754 -0.23556785\n  0.07306657  0.25215118  0.23204684  0.04892423 -0.12866025 -0.12747872\n  0.23613291 -0.14032689  0.27545584  0.06085061 -0.19938816  0.02115924\n -0.28282794  0.07081992 -0.1177591  -0.20459564  0.01877627 -0.25639449\n -0.03569688  0.1237346   0.2662399   0.1771565  -0.09440673 -0.08126141\n -0.17877506 -0.11151098  0.21644367  0.18695703  0.17999011 -0.17637867\n  0.1261223  -0.12128747 -0.09164908 -0.1335805   0.1349171  -0.08299479\n -0.26022463  0.10985452 -0.00252328  0.07477953  0.0306205  -0.04577449\n -0.04131809 -0.04008057 -0.01597525  0.05820192 -0.124408   -0.03692491\n  0.03629847 -0.28037821  0.25339746  0.21384497  0.26513572 -0.22788953\n  0.25432455 -0.18585833 -0.23787422 -0.08532806  0.01021225  0.07475056\n  0.20932279  0.19860247  0.13852761  0.24047614 -0.01878973  0.25372542\n -0.14279694 -0.09080622 -0.22740184  0.10680389  0.10907527 -0.21891534\n  0.20341292  0.09316087 -0.01780466  0.22026635 -0.09104243 -0.1065813\n  0.14714161  0.11700263  0.03462749 -0.20853976 -0.04464864 -0.17747381\n -0.27029921 -0.15788189 -0.31686121 -0.095694   -0.07252727 -0.24548785\n  0.2917659   0.0488352  -0.09200077 -0.10632164  0.20503777  0.13429535] 8.875979862972325e-05\ntarget values for D:  [1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1.\n 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0.\n 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0.\n 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.]\npredictions on D:  [ True  True False False  True  True False  True False False False False\n False False  True  True False  True  True  True False  True  True  True\n False False False False  True  True  True  True False False  True False\n  True  True False False False  True  True  True  True False False False\n  True  True False False False False  True  True False  True False  True\n  True False  True  True  True  True  True  True False False False  True\n False False False  True False  True  True False  True False  True  True\n  True False  True  True  True  True  True  True False  True False  True\n  True False False  True  True  True False False  True  True False False\n  True False False False False False  True  True  True False False False\n False False False False  True  True False  True False  True False  True\n False False False False False  True  True  True False False False  True\n  True  True False  True False  True  True False  True  True False False\n  True  True  True  True False  True False False  True  True  True  True\n False False False False False  True False  True  True False  True False\n  True  True  True  True  True  True False  True False False False False\n False  True False  True  True False False  True False False  True False\n False  True False  True False False False  True False False  True False\n False False  True  True False  True False False False False False False\n False  True  True False False False  True False False False  True False\n  True False False  True False  True  True  True  True  True  True False\n False  True False  True False  True  True  True False  True False False\n  True False False  True False False False  True False False False False\n  True  True  True False False  True False  True  True False False False\n False  True  True False  True False False  True  True False  True False\n False  True False False  True  True False  True False False  True False\n False False  True  True False  True  True False  True False  True False\n False  True  True False False False False False False  True  True False\n False  True  True  True  True False  True  True  True  True False False\n False False False  True  True False False False  True  True  True  True\n  True  True  True False  True  True  True  True  True False False False\n False False False False False False False False False  True False  True\n False False False False False False False False False  True  True  True\n  True  True False False]\n"
     ]
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "print (\"Final model:\")\n",
    "print (w_1.get_value(), b_1.get_value())\n",
    "print (w_2.get_value(), b_2.get_value())\n",
    "print (\"target values for D: \", D[1])\n",
    "print (\"predictions on D: \", predict(D[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 정확도 측정 (무작위로 데이터를 넣엇기 때문에 특징을 잡기가 어려워서 정확도가 낮을듯)\n",
    "import numpy as np\n",
    "print (np.float(np.sum(D[1]==predict(D[0]))) / np.float(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}